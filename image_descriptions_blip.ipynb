{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc8e51-c002-43fa-bd81-cec3bc2740ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import copy\n",
    "\n",
    "# Function to load the image from a local file\n",
    "def load_image(image_path):\n",
    "    return Image.open(image_path)\n",
    "\n",
    "# Example usage:\n",
    "image_path = 'YOUR IMAGE PATH.jpg'\n",
    "image = load_image(image_path)\n",
    "image.show()  # This will display the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65313318-3bfe-45fc-a4cb-34a331998e83",
   "metadata": {},
   "source": [
    "# Load the BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def generate_description_old(image):\n",
    "    # Preprocess the image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate a description\n",
    "    outputs = model.generate(**inputs)\n",
    "    description = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return description\n",
    "\n",
    "# Example usage:\n",
    "description = generate_description_old(image)\n",
    "print(\"Generated Description:\", description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0dc98-ef4a-4bc0-85d4-6bdf75778fb8",
   "metadata": {},
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, TrainingArguments, Trainer\n",
    "#from matplotlib import c\n",
    "\n",
    "fashion_mnist=load_dataset('fashion_mnist', download_mode='force_redownload', cache_dir=None)\n",
    "\n",
    "# # Load the full FashionMNIST train and test datasets if you want to subset from them\n",
    "#train_dataset = load_dataset('fashion_mnist', split='train', download_mode='force_redownload', cache_dir=None)\n",
    "#test_dataset = load_dataset('fashion_mnist', split='test', download_mode='force_redownload', cache_dir=None)\n",
    "\n",
    "# Select the first 30,000 rows from the training set\n",
    "#train_subset = train_dataset.select(range(10000))\n",
    "\n",
    "# Select the first 5,000 rows from the test set\n",
    "#test_subset = test_dataset.select(range(500))\n",
    "\n",
    "\n",
    "# fashion_mnist = DatasetDict({\n",
    "#     'train': train_subset,               # Assign the train subset\n",
    "#     'test': test_subset                # Assign the test subset\n",
    "#     #'validation': train_subset.select(range(5000))  # Optionally create a validation set\n",
    "# })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bdfbd9-48a9-4280-955a-fa1a0949f265",
   "metadata": {},
   "source": [
    "# Load the BLIP processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5983de79-9c88-47f8-9b56-8a3cdde344dd",
   "metadata": {},
   "source": [
    "#clear cache before training the model\n",
    "import torch\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42e0f1-08aa-433d-82e1-4ea12a6aa9b5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Load the Fashion-MNIST dataset\n",
    "import torch\n",
    "import PIL\n",
    "from torchvision import transforms\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/blip-image-captioning-base\")  # Replace with the model you are usin\n",
    "\n",
    "\n",
    "def ResizeMax(img, max_sz=256):\n",
    "    if isinstance(img, Image.Image):\n",
    "        img.thumbnail((max_sz, max_sz), Image.ANTIALIAS)\n",
    "        return img\n",
    "    else:\n",
    "        raise ValueError(\"Expected a PIL image\")\n",
    "        \n",
    "# Function to preprocess images and labels from Fashion-MNIST\n",
    "def preprocess_fashion_mnist(examples):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(np.uint8(x)).convert('RGB')), # Convert numpy array to PIL Image and to RGB\n",
    "        transforms.Resize((256, 256)), \n",
    "        transforms.PILToTensor()          # Convert PIL Image to Tensor\n",
    "    ])\n",
    "\n",
    "    # Apply the transformation directly\n",
    "    images = [transform(image) for image in examples['image']]\n",
    "\n",
    "    #Generate text\n",
    "    text_data = [\"A picture of a clothing item.\" for _ in range(len(examples['image']))]\n",
    "\n",
    "    # Process images using the BLIP processor\n",
    "    inputs = processor(images=images, text=text_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Ensure pixel_values is a tensor\n",
    "    pixel_values = inputs['pixel_values']\n",
    "\n",
    "    if isinstance(pixel_values, list):\n",
    "        pixel_values = torch.stack([torch.tensor(img).permute(1, 2, 0) for img in pixel_values])\n",
    "\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': torch.tensor(examples['label']),\n",
    "        'input_ids': input_ids  # Remove batch dimension if needed\n",
    "\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "fashion_mnist = fashion_mnist.map(preprocess_fashion_mnist, batched=True, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978166c-d71e-434d-8648-6b3a0c8c3981",
   "metadata": {},
   "source": [
    "# Create a deep copy of the DatasetDict to then set list back to tensor\n",
    "formatted_fashion_mnist = copy.deepcopy(fashion_mnist)\n",
    "formatted_fashion_mnist.set_format(type='pt', columns=['pixel_values', 'label','image'], output_all_columns=True)\n",
    "print(fashion_mnist)\n",
    "print(formatted_fashion_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ccec20-ccde-4933-9deb-a6141633dc88",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Do some checks on the output \n",
    "from torchvision import transforms\n",
    "\n",
    "print(type(formatted_fashion_mnist))\n",
    "\n",
    "# Access a sample from the 'train' split\n",
    "first_sample = formatted_fashion_mnist['train'][435]\n",
    "\n",
    "print(f\"First sample type: {type(first_sample)}\")\n",
    "print(f\"First sample keys: {list(first_sample.keys())}\")\n",
    "\n",
    "# # Check the type and content of pixel_values and labels\n",
    "pixel_values = first_sample['pixel_values']\n",
    "labels = first_sample['labels']\n",
    "label = first_sample['label']\n",
    "image=first_sample['image']\n",
    "\n",
    "#print(f\"Pixels type: {type(pixel_values)}\")\n",
    "#print(f\"Labels type: {type(labels)}\")\n",
    "#print(f\"Label type: {type(label)}\")\n",
    "#print(f\"Image is: {image}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7dece-59d7-4fe2-ba74-4c21f3e4d1e5",
   "metadata": {},
   "source": [
    "# Convert the pixels into image\n",
    "\n",
    "def display_image_from_pixel_values(pixel_values):\n",
    "    # Extract the first image from the batch\n",
    "    if pixel_values.ndim == 4:\n",
    "        pixel_values = pixel_values[0]  # Get the first image in the batch\n",
    "\n",
    "    # Check if pixel_values is normalized\n",
    "    if pixel_values.max() <= 1.0:\n",
    "        # Assuming values are normalized between 0 and 1\n",
    "        pixel_values = pixel_values * 255.0\n",
    "    \n",
    "    # Convert tensor to NumPy array and ensure values are in the 0-255 range\n",
    "    image_np = pixel_values.permute(1, 2, 0).byte().numpy()\n",
    "    \n",
    "    # Create a PIL Image from the NumPy array\n",
    "    image_pil = Image.fromarray(image_np)\n",
    "    \n",
    "    # Display the image\n",
    "    image_pil.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming `formatted_fashion_mnist` has been set up correctly\n",
    "first_sample = formatted_fashion_mnist['train'][5000]\n",
    "pixel_values = first_sample['pixel_values']\n",
    "\n",
    "# Display the image\n",
    "display_image_from_pixel_values(pixel_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a4a1b-ff68-408c-95e1-43369ff5b8c1",
   "metadata": {},
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        print('Computing loss...')\n",
    "        \n",
    "        # Extract pixel_values, input_ids, and labels\n",
    "        pixel_values = inputs.get('pixel_values')\n",
    "        input_ids = inputs.get('input_ids')\n",
    "        labels = inputs.get('input_ids')\n",
    "\n",
    "        # Debug statements to check types and shapes\n",
    "        print(f\"Pixel values type: {type(pixel_values)}, shape: {pixel_values.shape if pixel_values is not None else 'None'}\")\n",
    "        print(f\"Input IDs type: {type(input_ids)}, shape: {input_ids.shape if input_ids is not None else 'None'}\")\n",
    "        print(f\"Labels type: {type(labels)}, shape: {labels.shape if labels is not None else 'None'}\")\n",
    "        \n",
    "        # Check if pixel_values, input_ids, and labels are None\n",
    "        if pixel_values is None or input_ids is None or labels is None:\n",
    "            raise ValueError(\"Pixel values, input ids, or labels are None\")\n",
    "        \n",
    "        # Ensure the model is receiving the correct inputs\n",
    "        outputs = model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        # Extract loss\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "     output_dir=\"./blip-finetuned-fashion-mnist\",\n",
    "     bf16=True, #Enable mixed precision training (fp16)\n",
    "     per_device_train_batch_size=12,\n",
    "     #gradient_accumulation_steps=2,  # Accumulate gradients over steps to simulate a larger batch size\n",
    "     eval_strategy=\"steps\",\n",
    "  #   dataloader_num_workers=4,  # makes it slower for now\n",
    "     num_train_epochs=3,\n",
    "     save_steps=1000,\n",
    "     save_total_limit=2,\n",
    "     #remove_unused_columns=True,\n",
    "     remove_unused_columns=False,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_fashion_mnist['train'],\n",
    "    eval_dataset=formatted_fashion_mnist['test'],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ddb26-e89d-4217-8646-2db356df70e5",
   "metadata": {},
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned BLIP model and processor\n",
    "model_path = \"./blip-finetuned-fashion-mnist\"  # Path to the fine-tuned model\n",
    "processor = BlipProcessor.from_pretrained(model_path)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# Load processor and model from the final model directory (root)\n",
    "processor = BlipProcessor.from_pretrained(\"./blip-finetuned-fashion-mnist\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"./blip-finetuned-fashion-mnist\")\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to load and process an image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Open the image using PIL\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Preprocess the image with the processor\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    return inputs['pixel_values']\n",
    "\n",
    "# Function to generate description for an image\n",
    "def generate_description(image_path):\n",
    "    # Load and preprocess the image\n",
    "    pixel_values = load_and_preprocess_image(image_path)\n",
    "\n",
    "    # Generate description (caption)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    description = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return description\n",
    "\n",
    "# Example usage\n",
    "image_path = \"PATH_TO_IMAGE.jpg\"  # Replace with the path to the image you want to describe\n",
    "\n",
    "description = generate_description(image_path)\n",
    "print(\"Generated Description:\", description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beab5df-00ba-4179-9e93-ef698c2b2f20",
   "metadata": {},
   "source": [
    "#EXTRA CODE TO EXTRACT ATTRIBUTES TOO\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import spacy\n",
    "import webcolors\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the fine-tuned BLIP model and processor\n",
    "model_path = \"./blip-finetuned-fashion-mnist\"  # Path to the fine-tuned model\n",
    "processor = BlipProcessor.from_pretrained(model_path)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# Function to generate a description from an image using BLIP\n",
    "def generate_description(image_path):\n",
    "    # Load and preprocess the image\n",
    "    raw_image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Prepare inputs for the BLIP model\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate the description using the BLIP model\n",
    "    outputs = model.generate(**inputs)\n",
    "    description = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return description\n",
    "\n",
    "# Function to dynamically extract colors using webcolors\n",
    "def extract_colors(description):\n",
    "    # Common colors that can be extracted dynamically (using CSS21 standard)\n",
    "    color_names = set(webcolors.names(\"css3\"))   \n",
    "\n",
    "    # Find any color matches in the description\n",
    "    words = description.lower().split()\n",
    "    detected_colors = [word for word in words if word in color_names]\n",
    "    # If no specific colors found, return None\n",
    "    return \", \".join(set(detected_colors)) if detected_colors else None\n",
    "\n",
    "# Function to dynamically extract possible styles, fits, and other attributes using NLP\n",
    "def extract_attributes(description):\n",
    "    # Process the description with spaCy NLP\n",
    "    doc = nlp(description.lower())\n",
    "\n",
    "    attributes = {\n",
    "        \"Style\": None,\n",
    "        \"Color\": None,\n",
    "        \"Fit\": None,\n",
    "        \"Season\": None,\n",
    "        \"Brand\": None\n",
    "    }\n",
    "\n",
    "    # Extract adjectives and nouns which often describe style and fit\n",
    "    adjectives = [token.text for token in doc if token.pos_ == 'ADJ']\n",
    "    nouns = [token.text for token in doc if token.pos_ == 'NOUN']\n",
    "\n",
    "    # Combine adjectives and nouns as they are typically descriptive of the clothing\n",
    "    possible_styles = adjectives + nouns\n",
    "\n",
    "    # Set style (all adjectives/nouns can be considered style-related)\n",
    "    if possible_styles:\n",
    "        attributes[\"Style\"] = \", \".join(set(possible_styles))\n",
    "\n",
    "    # Extract color dynamically using the webcolors library\n",
    "    attributes[\"Color\"] = extract_colors(description)\n",
    "\n",
    "    # Fit extraction (search for common fitting-related words)\n",
    "    fit_keywords = [\"fitted\", \"loose\", \"tailored\", \"flared\", \"relaxed\", \"oversized\"]\n",
    "    detected_fits = [word for word in adjectives if word in fit_keywords]\n",
    "    if detected_fits:\n",
    "        attributes[\"Fit\"] = \", \".join(set(detected_fits))\n",
    "\n",
    "    # Season extraction based on common season words\n",
    "    if any(season in description.lower() for season in [\"spring\", \"summer\"]):\n",
    "        attributes[\"Season\"] = \"Spring, Summer\"\n",
    "    elif any(season in description.lower() for season in [\"fall\", \"winter\"]):\n",
    "        attributes[\"Season\"] = \"Fall, Winter\"\n",
    "\n",
    "    # Brand extraction - assumes brand is not mentioned unless explicitly found\n",
    "    attributes[\"Brand\"] = \"Specific brand not identified\"\n",
    "\n",
    "    return attributes\n",
    "\n",
    "# Example usage\n",
    "image_path = \"PATH_TO_IMAGE.jpg\"\n",
    "description = generate_description(image_path)\n",
    "attributes = extract_attributes(description)\n",
    "\n",
    "print(\"Description:\", description)\n",
    "print(\"Attributes:\", attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8af2f0-b03b-43cf-8039-e611334bd190",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
